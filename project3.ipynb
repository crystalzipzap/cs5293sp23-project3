{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2342813f",
   "metadata": {},
   "source": [
    "## Student Name: Chenyi \"Crystal\" Zhang\n",
    "## Student Email: cschmidt@ou.edu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e6ab65",
   "metadata": {},
   "source": [
    "# Project 3: The Smart City Slicker\n",
    "\n",
    "Imagine you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. In this project, you will perform \n",
    "exploratory data analysis, often shortened to EDA, to examine a data from the [2015 Smart City Challenge](https://www.transportation.gov/smartcity) to find facts about the data and communicating those facts through text analysis and visualizations.\n",
    "\n",
    "In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as data preprocessing or cleaning.\n",
    "Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring.\n",
    "Because of this tight coupling, you have to clean the data as necessary to help understand the data.\n",
    "\n",
    "In this project, you will apply your knowledge about data cleaning, machine learning, visualizations, and databases to explore smart city applications.\n",
    "\n",
    "**Part 1** of the notebook will explore and clean the data. \\\n",
    "**Part 2** will take the results of the preprocessed data to create models and visualizations.\n",
    "\n",
    "Empty cells are code cells. \n",
    "Cells denoted with [Your Answer Here] are markdown cells.\n",
    "Edit and add as many cells as needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e8dcba1",
   "metadata": {},
   "source": [
    "Output file for this notebook is shown as a table for display purposes. Note: The city name can be Norman, OK or OK Norman.\n",
    "\n",
    "| city | raw text | clean text | clusterid | topicids | \n",
    "| -- | -- | -- | -- | -- | \n",
    "|Norman, OK | Test, test , and testing. | test test test | 0 | T1, T2| "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89fd47ce",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Dataset: 2015 Smart City Challenge Applicants (non-finalist).\n",
    "In this project you will use the applicant's PDFs as a dataset.\n",
    "The dataset is from the U.S Department of Transportation Smart City Challenge.\n",
    "\n",
    "On the website page for the data, you can find some basic information about the challenge. This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n",
    "\n",
    "1. Can I identify frequently occurring words that could be removed during data preprocessing?\n",
    "2. Where are the applicants from?\n",
    "3. Are there multiple entries for the same city in different applicantions?\n",
    "4. What are the major themes and concepts from the smart city applicants?\n",
    "\n",
    "Let's load the data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aace966",
   "metadata": {},
   "source": [
    "## Loading and Handling files (Required)\n",
    "\n",
    "Load data from `smartcity/`. \n",
    "\n",
    "To extract the data from the pdf files, use the [pypdf.pdf.PdfFileReader](https://pypdf.readthedocs.io/en/stable/index.html) class.\n",
    "It will allow you to extract pages and pdf files and add them to a data structure (dataframe, list, dictionary, etc).\n",
    "To install the module, use the command `pipenv install pypdf`.\n",
    "You only need to handle PDF files, handling docx is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8c4358d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "715be583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ef0afb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path):\n",
    "    pdf_file = open(file_path, 'rb')\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    text = ''\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text() \n",
    "    pdf_file.close()\n",
    "    return text\n",
    "\n",
    "def extract_state_city_name(file):\n",
    "    if \"DC\" in file:\n",
    "        return \"DC\",  \"Washington, D.C\"\n",
    "    parts = file.split(' ')\n",
    "    pdf_extension_remove = parts[-1].split('.')\n",
    "    pdf_extension_remove.pop(-1)\n",
    "    state = parts[0]\n",
    "    del parts[0]\n",
    "    parts.pop(-1)\n",
    "    parts = parts + pdf_extension_remove\n",
    "    sep = ' '\n",
    "    city_name = sep.join(parts)\n",
    "    return state, city_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ed6e32",
   "metadata": {},
   "source": [
    "Create a data structure to add the city name and raw text. You can choose to split the city name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2e4905f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = os.path.join(os.getcwd(), 'smartcity')\n",
    "pdf_files = [file for file in os.listdir(files_path) if file.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "96f51f47",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of    State            City                                           Raw Text\n",
       "0     AK       Anchorage  CONTENTS \\n1 VISION .............................\n",
       "1     AL      Birmingham  aBirmingham\\nRising\\nBirmingham Rising! Meetin...\n",
       "2     AL      Montgomery   \\n \\n U.S. Department of Transportation - “BE...\n",
       "3     AZ   Scottsdale AZ    \\n  \\n \\n \\n \\nFederal Agency Name:   U.S. D...\n",
       "4     AZ          Tucson  Tucson Smart City Demonstration Proposal\\nPart...\n",
       "..   ...             ...                                                ...\n",
       "64    VA        Richmond    \\n \\n \\n   \\n \\n \\n  \\n      Contact Informa...\n",
       "65    VA  Virginia Beach    \\n1.  Project Vision  .........................\n",
       "66    WA         Seattle  Beyond Traffic: USDOT Smart City Challenge\\nAp...\n",
       "67    WA         Spokane  USDOT Smart City Challenge -  Spokane  \\nPage ...\n",
       "68    WI         Madison  Building a Smart Madison  \\nfor Shared Prosper...\n",
       "\n",
       "[69 rows x 3 columns]>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for pdf in pdf_files:\n",
    "    state, city = extract_state_city_name(pdf)\n",
    "    try:\n",
    "        raw_text = extract_text(os.path.join(files_path, pdf))\n",
    "        data.append([state, city, raw_text])\n",
    "    except:\n",
    "        print(pdf)\n",
    "df = pd.DataFrame(data, columns = ['State','City', 'Raw Text'])\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d820d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5019a8c3",
   "metadata": {},
   "source": [
    "## Cleaning Up PDFs (Required)\n",
    "\n",
    "\n",
    "One of the more frustrating aspects of PDF is loading the data into a readable format. The first order of business will be to preprocess the data. To start, you can use code provided by Text Analytics with Python, [Chapter 3](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb): [contractions.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/contractions.py) (Pages 136-137), and [text_normalizer.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/text_normalizer.py) (Pages 155-156). Feel free to download the scripts or add the code directly to the notebook (please note this code is performed on dataframes).\n",
    "\n",
    "In addition to the data cleaning provided by the textbook, you will need to:\n",
    "1. Consider removing terms that may effect clustering and topic modeling. Words to consider are cities, states, common words (smart, city, page, etc.). Keep in mind n-gram combinations are important; this can also be revisited later depending on your model's performance.\n",
    "2. Check the data to remove applicants that text was not processed correctly. Do not remove more than 15 cities from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8142e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "import unicodedata\n",
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "nlp = en_core_web_lg.load()\n",
    "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    if bool(soup.find()):\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    else:\n",
    "        stripped_text = text\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "#def correct_spellings_textblob(tokens):\n",
    "#\treturn [Word(token).correct() for token in tokens]  \n",
    "\n",
    "\n",
    "def simple_porter_stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=False, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, stopwords=stopword_list):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "\n",
    "        # remove extra newlines\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "\n",
    "        # stem text\n",
    "        if text_stemming and not text_lemmatization:\n",
    "        \tdoc = simple_porter_stemming(doc)\n",
    "\n",
    "        # remove special characters and\\or digits\n",
    "        #insert spaces between special characters to isolate them        \n",
    "        if special_char_removal:\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "         # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "76d3da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_most_common_words(text, n=10):\n",
    "    word_frequencies = Counter(text.split())\n",
    "    most_common_words = word_frequencies.most_common(n)\n",
    "    return [word for word, count in most_common_words]\n",
    "\n",
    "def remove_most_common_words(text, most_common_words):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in most_common_words]\n",
    "    return ' '.join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d58ff01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = [\"smart\", \"city\", \"page\", \"content\", \"appendix\", \"\"]\n",
    "city_names = df['City'].apply(lambda x: x.lower().split()).tolist()\n",
    "state_abbv = df['State'].apply(lambda x: x.lower().split()).tolist()\n",
    "state_names = [\n",
    "    'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida',\n",
    "    'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine',\n",
    "    'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska',\n",
    "    'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio',\n",
    "    'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas',\n",
    "    'utah', 'vermont', 'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming'\n",
    "]\n",
    "custom_stopwords.extend(city_names + state_names + state_abbv)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(custom_stopwords)\n",
    "df['Initial Cleaned Text'] = normalize_corpus(df['Raw Text'], stopwords=stopwords)\n",
    "df['Most Common Words'] = df['Initial Cleaned Text'].apply(get_most_common_words) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1473a3e3",
   "metadata": {},
   "source": [
    "#### Add the cleaned text to the structure you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "3737fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Final Cleaned Text'] = df.apply(lambda row: remove_most_common_words(row['Initial Cleaned Text'], row['Most Common Words']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82cc947b",
   "metadata": {},
   "source": [
    "### Clean Up: Discussion\n",
    "Answer the questions below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f1ba98d",
   "metadata": {},
   "source": [
    "#### Which Smart City applicants did you remove? What issues did you see with the documents?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffebf5a5",
   "metadata": {},
   "source": [
    "I removed `GA Columbus.docx` and `NM Albuquerque.docx` because my pipeline does not parse .docx documents. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1620ed74",
   "metadata": {},
   "source": [
    "#### Explain what additional text processing methods you used and why."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae42fc81",
   "metadata": {},
   "source": [
    "After reviewing some files, I came up with a general list of stopwords: `[\"smart\", \"city\", \"page\", \"content\", \"appendix\", \"\"]`.This list will be appended by the State and city name based on the file name. In addition, I also defined two functions `get_most_common_words()` and `remove_most_common_words()` to further clean the raw texts. After applying the `normalize_corpus()` function, a column of \"Most Common Words\" will be created and used to remove the most common words from the texts that went through `normalize_corpus()` - this would be appended to the dataframe as the \"Final Cleaned Text\" column.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15817355",
   "metadata": {},
   "source": [
    "#### Did you identify any potientally problematic words?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78df8030",
   "metadata": {},
   "source": [
    "Yes. The list of words that are all problematic are smart, city, page, content, appendix, and the unicode character ''. They have decent frequency but contribute little value in the upcoming analysis. This list is constant no matter the file. State and City names are already stored in the column so they can also be removed since their occurrence also contributes little to no value. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1507fbe",
   "metadata": {},
   "source": [
    "## Experimenting with Clustering Models (Required)\n",
    "\n",
    "Now, you'll start to explore models to find the optimal clustering model. In this section, you'll explore [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [Hierarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithms.\n",
    "Create these algorithms with k_clusters for K-means and Hierarchical.\n",
    "For each cell in the table provide the [Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), [Calinski and Harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score), and [Davies-Bouldin score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
    "\n",
    "In each cell, create an array to store the values.\n",
    "For example, \n",
    "\n",
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means| [S,CH,DB]| [S,CH,DB] | [S,CH,DB] | [S,CH,DB] |\n",
    "|Hierarchical |[S,CH,DB]| [S,CH,DB]| [S,CH,DB] | [S,CH,DB]|\n",
    "|DBSCAN | X | X | X | [S,CH,DB] |\n",
    "\n",
    "\n",
    "\n",
    "### Optimality \n",
    "You will need to find the optimal k for K-means and Hierarchical algorithms.\n",
    "Find the optimality for k in the range 2 to 50.\n",
    "Provide the code used to generate the optimal k and provide justification for your approach.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8da2b033",
   "metadata": {},
   "source": [
    "| Algorithm    | K = 9                     | K = 18                    | K = 36                   | Optimal k                 |\n",
    "| ------------ | ------------------------- | ------------------------- | ------------------------ | ------------------------- |\n",
    "| K-means      | [-0.0030, 1.2582, 1.8197] | [-0.0145, 1.2718, 1.4402] | [0.0083, 1.2775, 1.1046] | [0.0083, 1.2775, 1.1046]  |\n",
    "| Hierarchical | [-0.0109, 1.9876, 2.4133] | [0.0107, 1.7109, 1.8672]  | [0.0450, 1.6431, 1.1982] | [0.0450, 1.6431, 1.1982]  |\n",
    "| DBSCAN       |                           |                           |                          | [-0.0478, 2.5597, 1.3724] |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2dc8c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "fbd26981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(name, model, X):\n",
    "    X_dense = X.toarray()\n",
    "    labels = model.fit_predict(X_dense)\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        silhouette = silhouette_score(X_dense, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(X_dense, labels)\n",
    "        davies_bouldin = davies_bouldin_score(X_dense, labels)\n",
    "    else:\n",
    "        silhouette = calinski_harabasz = davies_bouldin = np.nan\n",
    "\n",
    "    print(f'{name}: S, CH, DB')\n",
    "    print(f'[{silhouette:.4f}, {calinski_harabasz:.4f}, {davies_bouldin:.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "0d96d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans (k=9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans: S, CH, DB\n",
      "[-0.0030, 1.2582, 1.8197]\n",
      "KMeans (k=18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans: S, CH, DB\n",
      "[-0.0145, 1.2718, 1.4402]\n",
      "KMeans (k=36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans: S, CH, DB\n",
      "[0.0083, 1.2775, 1.1046]\n",
      "Hierarchical (k=9)\n",
      "Hierarchical: S, CH, DB\n",
      "[-0.0109, 1.9876, 2.4133]\n",
      "Hierarchical (k=18)\n",
      "Hierarchical: S, CH, DB\n",
      "[0.0107, 1.7109, 1.8672]\n",
      "Hierarchical (k=36)\n",
      "Hierarchical: S, CH, DB\n",
      "[0.0450, 1.6431, 1.1982]\n",
      "DBSCAN\n",
      "DBSCAN: S, CH, DB\n",
      "[-0.0478, 2.5597, 1.3724]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Final Cleaned Text'])\n",
    "k_values = [9, 18, 36]\n",
    "models = {\n",
    "    'KMeans': [KMeans(n_clusters=k) for k in k_values],\n",
    "    'Hierarchical': [AgglomerativeClustering(n_clusters=k) for k in k_values],\n",
    "    'DBSCAN': [DBSCAN()]\n",
    "}\n",
    "\n",
    "for name, model_list in models.items():\n",
    "    for i, model in enumerate(model_list):\n",
    "        if name != 'DBSCAN':\n",
    "            print(f\"{name} (k={k_values[i]})\")\n",
    "        else:\n",
    "            print(f\"{name}\")\n",
    "        compute_metrics(name, model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans: Optimal K = 36\n",
      "Hierarchical: Optimal K = 36\n",
      "DBSCAN: Optimal K = 9\n"
     ]
    }
   ],
   "source": [
    "for name, model_list in models.items():\n",
    "    silhouette_scores = []\n",
    "    for model in model_list:\n",
    "        X_dense = X.toarray()\n",
    "        labels = model.fit_predict(X_dense)\n",
    "        silhouette = silhouette_score(X_dense, labels)\n",
    "        silhouette_scores.append(silhouette)\n",
    "        \n",
    "    optimal_k = k_values[np.argmax(silhouette_scores)]\n",
    "    print(f\"{name}: Optimal K = {optimal_k}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c20fd69",
   "metadata": {},
   "source": [
    "#### How did you approach finding the optimal k?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "753c54da",
   "metadata": {},
   "source": [
    "My approach is to use the use the silhouette score as the benchmark. The number of clusters that produces the highest Silhouette score is the most appropriate one. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f79ec635",
   "metadata": {},
   "source": [
    "#### What algorithm do you believe is the best? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3136b0a4",
   "metadata": {},
   "source": [
    "Based on the chart above, I believe Hierarchical model with 36 clusters is the best overall because it produced the highest Silhouette score and the Calinski-Harabasz and Davies-Bouldin are all not the lowest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e45a2a3",
   "metadata": {},
   "source": [
    "### Add Cluster ID to output file\n",
    "In your data structure, add the cluster id for each smart city respectively. Show the to append the clusterid code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "9ad83e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_model = AgglomerativeClustering(n_clusters=36)\n",
    "hierarchical_labels = hierarchical_model.fit_predict(X.toarray())\n",
    "df['Cluster ID'] = hierarchical_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959e7275",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After finding the best model, it is desirable to have a way to persist the model for future use without having to retrain. Save the model using [model persistance](https://scikit-learn.org/stable/model_persistence.html). This model should be saved in the same directory as this notebook and should be loaded as the model for your `project3.py`.\n",
    "\n",
    "Save the model as `model.pkl`. You do not have to use pickle, but be sure to save the persistance using one of the methods listed in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "1c80938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(hierarchical_model, f, protocol=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fe5a0c9",
   "metadata": {},
   "source": [
    "## Derving Themes and Concepts (Required)\n",
    "\n",
    "Perform Topic Modeling on the cleaned data. Provide the top five words for `TOPIC_NUM = Best_k` as defined in the section above. Feel free to reference [Chapter 6](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models) for more information on Topic Modeling and Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b684bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "915516dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    u, s, vt = svds(matrix, k=singular_count)\n",
    "    return u, s, vt\n",
    "\n",
    "def get_top_words_for_topic(vt, feature_names, num_top_words=5):\n",
    "    top_words_indices = (-vt).argsort()[:, :num_top_words]\n",
    "    top_words = [[feature_names[index] for index in topic] for topic in top_words_indices]\n",
    "    return top_words\n",
    "\n",
    "def remove_city_state_names(top_words, city_names, state_names):\n",
    "    city_state_names = [name.lower() for name in city_names + state_names]\n",
    "    filtered_top_words = [word for word in top_words if word not in city_state_names]\n",
    "    return filtered_top_words\n",
    "\n",
    "def correct_words(words):\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        doc = nlp(word)\n",
    "        if len(doc) > 0 and doc[0].has_vector:\n",
    "            token = doc[0].text\n",
    "            corrected_words.append(token)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5b5cb3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: service, mobility, indy\n",
      "Topic 2: anchorages\n",
      "Topic 3: lmg, tarc\n",
      "Topic 4: new\n",
      "Topic 5: linc, time, dade\n",
      "Topic 6: jax, dart, applicant, jea\n",
      "Topic 7: pinellas, time, pg\n",
      "Topic 8: madisons, uw, time, prosperity, feb\n",
      "Topic 9: dade, linc, okc\n",
      "Topic 10: lmg, mdot, linc, tarc\n",
      "Topic 11: dade, jax, lmg, vision, service\n",
      "Topic 12: okc, dade, would\n",
      "Topic 13: uw, madisons, traffic, information\n",
      "Topic 14: applicant, nys, dade, vdot\n",
      "Topic 15: marta, madisons, uw\n",
      "Topic 16: area, niagara\n",
      "Topic 17: pinellas, nys, schenectady\n",
      "Topic 18: traffic, icar, vision, pinellas\n",
      "Topic 19: pinellas, parking, davidson, downtown\n",
      "Topic 20: provide, traffic, figure\n",
      "Topic 21: technology, vehicle, pinellas, service\n",
      "Topic 22: public, project, system, dart\n",
      "Topic 23: public, pinellas, dade, county\n",
      "Topic 24: transit, public, traffic, metro, firestone\n",
      "Topic 25: project, challenge, traffic, nys\n",
      "Topic 26: traffic, transit, datum, niagara, project\n",
      "Topic 27: vehicle, hampton, dade, vdot\n",
      "Topic 28: vehicle, marta, provide, demonstration\n",
      "Topic 29: use, service, transit\n",
      "Topic 30: technology, transportation, datum, service\n",
      "Topic 31: datum, technology, transit, metro\n",
      "Topic 32: transportation, use, technology, transit\n",
      "Topic 33: datum, use, applicant\n",
      "Topic 34: system, bart, time, transportation, bay\n",
      "Topic 35: network, transit, information, provide, management\n",
      "Topic 36: prima, shoul\n"
     ]
    }
   ],
   "source": [
    "num_topics = 36\n",
    "u, s, vt = low_rank_svd(X.toarray(), singular_count=num_topics)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words_per_topic = get_top_words_for_topic(vt, feature_names, num_top_words=5)\n",
    "\n",
    "city_names = df['City'].unique().tolist()\n",
    "state_names = df['State'].unique().tolist()\n",
    "filtered_and_corrected_top_words = []\n",
    "\n",
    "for i, top_words in enumerate(top_words_per_topic):\n",
    "    filtered_words = remove_city_state_names(top_words, city_names, state_names)\n",
    "    corrected_words = correct_words(filtered_words)\n",
    "    filtered_and_corrected_top_words.append(corrected_words)\n",
    "\n",
    "for i, top_words in enumerate(filtered_and_corrected_top_words):\n",
    "    print(f\"Topic {i+1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1573fe65",
   "metadata": {},
   "source": [
    "### Extract themes\n",
    "Write a theme for each topic (atleast a sentence each)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9737d8df",
   "metadata": {},
   "source": [
    "| Unique Topics | Keywords                                           | Theme                                                                                                 |\n",
    "| ------------- | -------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| Topic 1       | service, mobility, indy                            | Innovative services that help mobility is important                                                   |\n",
    "| Topic 2       | madisons, uw, vision, pg                           | Universal collaboration can be an essential way to improve city planning.                             |\n",
    "| Topic 3       | linc, davidson, clair, niagara                     | Different region's collaboration is key to succeed the goal of smart city.                            |\n",
    "| Topic 4       | Dade, Cleveland's                                  | Cleveland is a great example of accessibility on the road for people who walks slow.                  |\n",
    "| Topic 5       | dart, city's, reimagine                            | A key part of achieving smart city is to reimagine public transportation.                             |\n",
    "| Topic 6       | Jax, dart, applicant, jea                          | Energy plays an essential role in public transportation.                                              |\n",
    "| Topic 7       | pinellas, time, pg                                 | Transportation improves travel time.                                                                  |\n",
    "| Topic 8       | madisons, uw, time, prosperity, feb                | University of Wisconsin contributed to the prosperity of smart city in February (not sure which year) |\n",
    "| Topic 9       | Jax, lmg, mdot                                     | Transportation planning in various cities play an important role to reach the goal of smart city.     |\n",
    "| Topic 10      | time, davidson, public                             | Public transportation in Davidson county saves time.                                                  |\n",
    "| Topic 11      | Dade, Jax, lmg, vision, service                    | Accessibility is a long-term vision in Jacksonville.                                                  |\n",
    "| Topic 12      | dart, applicant, datum, bostons, clair             | Data-driven solutions benefited Boston's public transportation.                                       |\n",
    "| Topic 13      | uw, madisons, traffic, information                 | University of Wisconsin contributed great information on traffic control.                             |\n",
    "| Topic 14      | applicant, nys, Dade, vdot                         | New York's public transportation benefited from several factors.                                      |\n",
    "| Topic 15      | marta, madisons, uw                                | University of Wisconsin contributed to public transportation's growth.                                |\n",
    "| Topic 16      | okc, pima, pag                                     | Oklahoma City's Pima Association of Governments plays an important role in achieving smart city.      |\n",
    "| Topic 17      | Cleveland's, linc, okc, madisons                   | University of Wisconsin assisted Cleveland and Oklahoma City to achieve better public transportation. |\n",
    "| Topic 18      | traffic, icar, vision, pinellas                    | Traffic management benefits from innovation.                                                          |\n",
    "| Topic 19      | pinellas, parking, davidson, downtown              | Downtown parking stress could benefit from smart city.                                                |\n",
    "| Topic 20      | provide, traffic, figure                           | Efficient transportation solutions can be provided by smart city plannings.                           |\n",
    "| Topic 21      | technology, vehicle, pinellas, service             | Smart vehicle and services could also help achieving the goal of smart city.                          |\n",
    "| Topic 22      | bart, pinellas, service, traffic                   | Public transit and traffic management could benefit from smart city.                                  |\n",
    "| Topic 23      | public, pinellas, Dade, county                     | public transportation in Pinellas County is hoping to make more improvement in accessibility.         |\n",
    "| Topic 24      | data, service, new, transportation                 | Data-driven solution can benefit transportation services.                                             |\n",
    "| Topic 25      | transit, datum, marta, feb                         | Transit data is very valuable.                                                                        |\n",
    "| Topic 26      | traffic, transit, datum, niagara, project          | Traffic and transit project management in Niagara county is important.                                |\n",
    "| Topic 27      | firestoneu use, deck, bowery, kenmore              | Several city's development and infrastructure plays an important role in smart city visions.          |\n",
    "| Topic 28      | use, system, bart, transit, public                 | Public transportation system's usage is essential.                                                    |\n",
    "| Topic 29      | use, service, transit                              | Usage of transit services can be beneficial to smart city.                                            |\n",
    "| Topic 30      | technology, transportation, datum, service         | Transportation's tech advancement is data driven.                                                     |\n",
    "| Topic 31      | use, vehicle, traffic, uos, rhode                  | Self-driven vehicle's usage impacted traffic in Rhode Island.                                         |\n",
    "| Topic 32      | transportation, use, technology, transit           | Innovative transportation solutions improve transit's usage.                                          |\n",
    "| Topic 33      | transit, transportation, onal, innova, uab         | Innovative transportation and collaboration play an important role in achieving smart city goal.      |\n",
    "| Topic 34      | system, bart, time, transportation, bay            | Public transportation in bay area relies on a timely system.                                          |\n",
    "| Topic 35      | network, transit, information, provide, management | Transit network and information provide ways for stakeholders to manage the load.                     |\n",
    "| Topic 36      | provide, include, public, use, service             | Public transportation promotes inclusive use of services                                              |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94ab7df0",
   "metadata": {},
   "source": [
    "### Add Topid ID to output file\n",
    "Add the top two topics for each smart city to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "8e568652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster ID'] = hierarchical_labels\n",
    "\n",
    "cluster_topic_scores = np.zeros((len(np.unique(hierarchical_labels)), num_topics))\n",
    "\n",
    "for cluster_id in range(len(np.unique(hierarchical_labels))):\n",
    "    cluster_cities_indices = np.where(hierarchical_labels == cluster_id)\n",
    "    cluster_topic_scores[cluster_id] = u[cluster_cities_indices].mean(axis=0)\n",
    "\n",
    "top_two_topics_df = pd.DataFrame(columns=['Top Topic 1', 'Top Topic 2'])\n",
    "\n",
    "for city_index in range(df.shape[0]):\n",
    "    cluster_id = df.loc[city_index, 'Cluster ID']\n",
    "    cluster_topics = cluster_topic_scores[cluster_id]\n",
    "    sorted_topics_indices = np.argsort(cluster_topics)[::-1][:2]\n",
    "    top_two_topics = [filtered_and_corrected_top_words[topic] for topic in sorted_topics_indices]\n",
    "    top_two_topics_df.loc[city_index] = [', '.join(top_two_topics[0]), ', '.join(top_two_topics[1])]\n",
    "\n",
    "df['Top Topic 1'] = top_two_topics_df['Top Topic 1']\n",
    "df['Top Topic 2'] = top_two_topics_df['Top Topic 2']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f9cb2c4",
   "metadata": {},
   "source": [
    "## Write output data (Required)\n",
    "\n",
    "The output data should be written as a TSV file.\n",
    "You can use `to_csv` method from Pandas for this if you are using a DataFrame.\n",
    "\n",
    "`Syntax: df.to_csv('file.tsv', sep = '')` \\\n",
    "`df.to_csv('smartcity_eda.tsv', sep='\\t')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "58827464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('smartcity_eda.csv', index=False, escapechar='ᚦ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a18b8ff",
   "metadata": {},
   "source": [
    "# Moving Forward\n",
    "Now that you have explored the dataset, take the important features and functions to create your `project3.py`.\n",
    "Please refer to the project spec for more guidance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a6675ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
